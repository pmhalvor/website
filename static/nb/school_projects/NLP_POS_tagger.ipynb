{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 \n",
    "### 1a\n",
    "We will simplify and use the universal pos tagset in this exercise to make\n",
    "the experiments run faster.\n",
    "\n",
    "We will be a little more cautious than the NLTK-book, when it comes to training and test sets.\n",
    "We will:\n",
    "- Split the News-section into three sets:\n",
    "    - 10% for final testing which we tuck aside for now, call it news_test\n",
    "    - 10% for development testing, call it news_dev_test\n",
    "    - 80% for training, call it news_train\n",
    "- Make the data sets, and repeat the training and evaluation with news_train and news_dev_test.\n",
    "- Use 4 counting decimal places and stick to that throughout the exercise set.\n",
    "\n",
    "How is the result compared to using the full brown tagset? Why do you think one of the tagsets\n",
    "yields higher scores than the other one?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "tagged_sents = brown.tagged_sents(categories='news') # read in corups as sentences\n",
    "# Shoulnd't we randomize these?\n",
    "\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/10/00: 4161 462\n",
      "80/10/10: 3699 462 462\n"
     ]
    }
   ],
   "source": [
    "news_train = tagged_sents[:-2*size] \n",
    "news_dev_test = tagged_sents[-2*size:-size]\n",
    "news_final_test = tagged_sents[-size:]\n",
    "\n",
    "print('90/10/00:', len(train_sents), len(test_sents))\n",
    "print('80/10/10:', len(news_train), len(news_dev_test), len(news_final_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"Atlanta's\", 'NP$'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'NN'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('``', '``'),\n",
       " ('no', 'AT'),\n",
       " ('evidence', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('that', 'CS'),\n",
       " ('any', 'DTI'),\n",
       " ('irregularities', 'NNS'),\n",
       " ('took', 'VBD'),\n",
       " ('place', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train[1][-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    '''\n",
    "    Takes in a sentence and index and finds the 3 preceeding letters.\n",
    "    \n",
    "    The idea here is to look at how this word ends versus the context of the previous word. \n",
    "    \n",
    "    History is currently not used.\n",
    "    \n",
    "    '''\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"      # The start of this sentence set\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]  # Why are we returning the whole sentence here? \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutivePosTagger(nltk.TaggerI):   # Inherits attributes from TaggerI\n",
    "    '''\n",
    "    \n",
    "    [TaggerI package](https://www.nltk.org/api/nltk.tag.html?highlight=taggeri#nltk.tag.api.TaggerI)\n",
    "    evaluate(gold)[source]\n",
    "        Score the accuracy of the tagger against the gold standard. Strip the tags from the gold standard text, retag it using the tagger, then compute the accuracy score.\n",
    "\n",
    "        Parameters\n",
    "        gold (list(list(tuple(str, str)))) – The list of tagged sentences to score the tagger on.\n",
    "\n",
    "        Return type\n",
    "        float - the score of that test set.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, train_sents, features=pos_features):\n",
    "        self.features = features\n",
    "        train_set = []\n",
    "        for tagged_sent in tqdm(train_sents):             # Singling out each sentence\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)   # Untagged sentence for words as list\n",
    "            history = []                                  # Stores the tags for each word (i.e. word type)\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history) # This is a call to pos_features\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history) # This is a call to pos_features\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4161/4161 [00:00<00:00, 5196.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7915\n"
     ]
    }
   ],
   "source": [
    "old_tagger = ConsecutivePosTagger(train_sents)\n",
    "print(round(old_tagger.evaluate(test_sents), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 3699/3699 [00:00<00:00, 4661.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7653\n"
     ]
    }
   ],
   "source": [
    "new_tagger = ConsecutivePosTagger(news_train)\n",
    "print(round(new_tagger.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model is built using fewer data points, it makes sense that it gives a slightly lower accuracy score than the model built on the full brown corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b\n",
    "\n",
    "One of the first things we should do in an experiment like this, is to establish a reasonable baseline.\n",
    "\n",
    "A reasonable baseline here is the <mark>Most Frequent Class</mark> baseline. \n",
    "Each word which is seen during training should get its most frequent tag from the training.  \n",
    "For words not seen during training, we simply use the most frequent overall tag.\n",
    "With news_train as training set and news_dev_set as valuation set, what is the accuracy of this\n",
    "baseline?\n",
    "\n",
    "Does the tagger from part (a) using the features from the NLTK book beat the baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FROM NLTK](https://www.nltk.org/api/nltk.tag.html?highlight=most%20frequent)\n",
    "\n",
    "_This package defines several taggers, which take a list of tokens, assign a tag to each one, and return the resulting list of tagged tokens. Most of the taggers are built automatically based on a training corpus. For example, the unigram tagger tags each word w by checking what the most frequent tag for w was in a training corpus:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure I split the tags properly, I will use a frequency distribution to count the tags. \n",
    "\n",
    "From Wikipedia:\n",
    "_Note that some versions of the tagged Brown corpus contain combined tags. For instance the word \"wanna\" is tagged VB+TO, since it is a contracted form of the two words, want/VB and to/TO. Also some tags might be negated, for instance \"aren't\" would be tagged \"BER*\", where * signifies the negation. Additionally, tags may have hyphenations: The tag -HL is hyphenated to the regular tags of words in headlines. The tag -TL is hyphenated to the regular tags of words in titles. The hyphenation -NC signifies an emphasized word. Sometimes the tag has a FW- prefix which means foreign word._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the five most common tags are [('NN', 10656), ('IN', 8404), ('AT', 7080), ('NP', 5859), (',', 3976)]\n"
     ]
    }
   ],
   "source": [
    "pos_counts = nltk.FreqDist([tag\n",
    "                            for sentence in news_train\n",
    "                            for (word, tag) in sentence])\n",
    "\n",
    "print(\"the five most common tags are\", pos_counts.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts.most_common(1)[0][0]\n",
    "most_common = pos_counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "baseline_news_tagger = UnigramTagger(news_train)\n",
    "\n",
    "baseline_news = [\n",
    "    [(word, tag) if tag!=None else (word, most_common)\n",
    "     for word, tag in baseline_news_tagger.tag(nltk.tag.untag(sentence))]\n",
    "     for sentence in news_dev_test \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(len(news_dev_test)):\n",
    "    for w in range(len(news_dev_test[i])):\n",
    "        if news_dev_test[i][w] == baseline_news[i][w]:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline accuracy is: 0.8268\n"
     ]
    }
   ],
   "source": [
    "print('The baseline accuracy is:', round(np.mean(results),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model gave an accuracy of 0.7653, and therefore does _not_ beat this baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Our goal will now be to improve the tagger compared to the simple suffix-based tagger. For the further experiments, we move to scikit-learn which yields more options for considering various alternatives. We have reimplemented the ConsecutivePosTagger to use scikit-learn classifiers below. We have made the classifier a parameter so that it can easily be exchanged. We start with the BernoulliNB-classifier which should correspond to the way it is done in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class ScikitConsecutivePosTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents, \n",
    "                 features=pos_features, clf = BernoulliNB()):\n",
    "        # Using pos_features as default.\n",
    "        self.features = features\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        for tagged_sent in tqdm(train_sents):\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_features.append(featureset)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        v = DictVectorizer()\n",
    "        X_train = v.fit_transform(train_features)\n",
    "        y_train = np.array(train_labels)\n",
    "        \n",
    "        # For help in exercise 5\n",
    "        try:\n",
    "            clf.fit(X_train, y_train)\n",
    "        except MemoryError:\n",
    "            print('Memory error. Attempting partial fit...')\n",
    "            clf.partial_fit(X_train, y_train, n_jobs=-2)\n",
    "            for n in range(100):    # arbitrarily chose this range\n",
    "                batch_indexs = np.random.sample(range(2000), 20)\n",
    "                clf.partial_fit(X_train[batch_indexes, :], y_train[batch_indexes])\n",
    "        \n",
    "        \n",
    "        self.classifier = clf\n",
    "        self.dict = v\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        test_features = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            test_features.append(featureset)\n",
    "        X_test = self.dict.transform(test_features)\n",
    "        tags = self.classifier.predict(X_test)\n",
    "        return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2a.\n",
    "In this part, we will train the `ScikitConsecutivePosTagger` on the `news_train` set and test on the `news_dev_test` set with the `pos_features`, to see if we get the same result as in exercise 1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 3699/3699 [00:00<00:00, 5589.83it/s]\n"
     ]
    }
   ],
   "source": [
    "sklearn_tagger = ScikitConsecutivePosTagger(news_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5853"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(sklearn_tagger.evaluate(news_dev_test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b.\n",
    "I get inferior results compared to using the NLTK set-up with the same feature extractors. The only explanation I could find is that the smoothing is too strong. `BernoulliNB()` from scikit-learn uses Laplace smoothing as default `(\"add-one\")`. The smoothing is generalized to Lidstone smoothing which is expressed by the alpha parameter to `BernoulliNB(alpha=...)` Therefore, we can tune this hyper-parameter with different `alpha`s; specifically `[1, 0.5, 0.1, 0.01, 0.001, 0.0001]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 3699/3699 [00:00<00:00, 6043.90it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 3699/3699 [00:00<00:00, 7772.06it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 3699/3699 [00:00<00:00, 5786.51it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 3699/3699 [00:00<00:00, 5803.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 3699/3699 [00:00<00:00, 5368.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 3699/3699 [00:00<00:00, 4638.10it/s]\n"
     ]
    }
   ],
   "source": [
    "alphas =  [1, 0.5, 0.1, 0.01, 0.001, 0.0001]\n",
    "scores = []\n",
    "for a in alphas:\n",
    "    sklearn_tagger = ScikitConsecutivePosTagger(news_train, clf= BernoulliNB(alpha=a))\n",
    "    scores.append(round(sklearn_tagger.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha 0.1 with score 0.7664\n"
     ]
    }
   ],
   "source": [
    "best = [z for z in zip(alphas, scores) if z[1]==max(scores)][0]\n",
    "print(f'Best alpha {best[0]} with score {best[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5853, 0.6811, 0.7664, 0.7631, 0.7419, 0.7348]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores # All of the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2c.\n",
    "To improve the results, we may change the feature selector. This is which attributes we are pulling from the sentences. So far, in `pos_features`, we include both the previous word, as well as the last 1, 2 and 3 preceding letters as the features to predict each word.\n",
    "\n",
    "We start with a simple improvement of our feature selector. In addition the previous word, we will expand our feature selector to also contain the word itself. Intuitively, the word itself should be a stronger feature than the previous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanded_pos_features(sentence, i, history):\n",
    "    '''\n",
    "    Takes in a sentence and index and finds the 3 preceeding letters.\n",
    "    \n",
    "    The idea here is to look at how this word ends versus the context of the previous word. \n",
    "    \n",
    "    History is currently not used.\n",
    "    \n",
    "    '''\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"      # The start of this sentence set\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]  # Why are we returning the whole sentence here? \n",
    "    \n",
    "    features['token'] = sentence[i]            # add word as feature to data\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new feature selector, we can rerun the experiment with the various `alphas` and record the results. \n",
    "\n",
    "It will be particularly interesting to see if we get the same `alpha` as above, along with the changes in the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/6 [00:00<?, ?it/s]\n",
      "  0%|                                                                                      | 0/3699 [00:00<?, ?it/s]\n",
      " 35%|█████████████████████████▎                                              | 1303/3699 [00:00<00:00, 12901.83it/s]\n",
      " 71%|███████████████████████████████████████████████████▎                    | 2635/3699 [00:00<00:00, 12986.40it/s]\n",
      " 17%|█████████████▌                                                                   | 1/6 [00:51<04:19, 51.83s/it]\n",
      "  0%|                                                                                      | 0/3699 [00:00<?, ?it/s]\n",
      " 29%|████████████████████▊                                                   | 1068/3699 [00:00<00:00, 10574.57it/s]\n",
      " 67%|████████████████████████████████████████████████                        | 2466/3699 [00:00<00:00, 11380.36it/s]\n",
      " 33%|███████████████████████████                                                      | 2/6 [01:43<03:27, 51.89s/it]\n",
      "  0%|                                                                                      | 0/3699 [00:00<?, ?it/s]\n",
      " 33%|███████████████████████▊                                                | 1224/3699 [00:00<00:00, 12118.98it/s]\n",
      " 70%|██████████████████████████████████████████████████▍                     | 2591/3699 [00:00<00:00, 12544.82it/s]\n",
      " 50%|████████████████████████████████████████▌                                        | 3/6 [02:35<02:35, 51.89s/it]\n",
      "  0%|                                                                                      | 0/3699 [00:00<?, ?it/s]\n",
      " 28%|███████████████████▉                                                    | 1027/3699 [00:00<00:00, 10168.43it/s]\n",
      " 63%|█████████████████████████████████████████████▌                          | 2339/3699 [00:00<00:00, 10903.41it/s]\n",
      " 67%|██████████████████████████████████████████████████████                           | 4/6 [03:27<01:43, 51.77s/it]\n",
      "  0%|                                                                                      | 0/3699 [00:00<?, ?it/s]\n",
      " 27%|███████████████████▉                                                      | 999/3699 [00:00<00:00, 9891.29it/s]\n",
      " 61%|████████████████████████████████████████████▎                           | 2274/3699 [00:00<00:00, 10604.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████▉| 3695/3699 [00:00<00:00, 11450.56it/s]\n",
      " 83%|███████████████████████████████████████████████████████████████████▌             | 5/6 [04:19<00:51, 51.97s/it]\n",
      "  0%|                                                                                      | 0/3699 [00:00<?, ?it/s]\n",
      " 31%|██████████████████████▎                                                 | 1146/3699 [00:00<00:00, 11459.90it/s]\n",
      " 69%|█████████████████████████████████████████████████▌                      | 2548/3699 [00:00<00:00, 12092.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 6/6 [05:11<00:00, 51.79s/it]\n"
     ]
    }
   ],
   "source": [
    "expanded_scores = []\n",
    "for a in tqdm(alphas):\n",
    "    sklearn_tagger = ScikitConsecutivePosTagger(news_train, features=expanded_pos_features, clf= BernoulliNB(alpha=a))\n",
    "    expanded_scores.append(round(sklearn_tagger.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5603, 0.7143, 0.8416, 0.8436, 0.8348, 0.8321]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha 0.01 with score 0.8436\n"
     ]
    }
   ],
   "source": [
    "expanded_best = [z for z in zip(alphas, expanded_scores) if z[1]==max(expanded_scores)][0]\n",
    "print(f'Best alpha {expanded_best[0]} with score {expanded_best[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we got a slightly different `alpha` than before, even though the alpha from above came in a close second. Since all of these scores seem higher than above, it is safe to conclude that our model is better off including the token as well as the previous features.\n",
    "\n",
    "We also got a score slightly better than the baseline here. \n",
    "\n",
    "\n",
    "\n",
    "__(EXPLAIN WHY THESE SHOULD THEORETICALLY BE THE SAME!!!)__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 3: Logistic regression (10 points)\n",
    "### Part a.\n",
    "We proceed with the best feature selector from the last exercise. \n",
    "\n",
    "We want to study the effect of the learner. \n",
    "\n",
    "We start by importing `LogisticRegression` to it instead of `BernoulliNB` as the `clf` parameter in our sklearn-tagger. We can then train again on `news_train` and test on `news_dev_test`, recording the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.886"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_tagger = ScikitConsecutivePosTagger(news_train, features=expanded_pos_features, clf= LogisticRegression())\n",
    "round(logreg_tagger.evaluate(news_dev_test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already scores better than the optimally tuned Bernoulli Naive Bayes, beating it by a whole 4 percent points! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b.\n",
    "Similarly to the Naive Bayes classifier, the logistic regression method can be tuned for our dataset. Smoothing for LogisticRegression is done by _regularization_. In scikit-learn, regularization is expressed by the parameter C. A smaller C means a heavier smoothing. (C is the inverse of the parameter $\\alpha$ in the lectures.) It will be interesting to see if we can get this same theoretical value for C in practice.\n",
    "\n",
    "We will try with C in range [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] and see which value yields the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]C:\\Users\\perha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\n",
      " 17%|█████████████▊                                                                     | 1/6 [02:38<13:11, 158.26s/it]C:\\Users\\perha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\n",
      " 33%|███████████████████████████▋                                                       | 2/6 [05:20<10:37, 159.41s/it]C:\\Users\\perha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\n",
      " 50%|█████████████████████████████████████████▌                                         | 3/6 [08:05<08:03, 161.15s/it]C:\\Users\\perha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\n",
      " 67%|███████████████████████████████████████████████████████▎                           | 4/6 [10:51<05:24, 162.47s/it]C:\\Users\\perha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\n",
      " 83%|█████████████████████████████████████████████████████████████████████▏             | 5/6 [13:33<02:42, 162.58s/it]C:\\Users\\perha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [16:19<00:00, 163.42s/it]"
     ]
    }
   ],
   "source": [
    "C = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "logreg_scores = []\n",
    "for c in tqdm(C):\n",
    "    sklearn_tagger = ScikitConsecutivePosTagger(news_train, features=expanded_pos_features, clf= LogisticRegression(C=c))\n",
    "    logreg_scores.append(round(sklearn_tagger.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6747, 0.8348, 0.886, 0.8963, 0.8954, 0.8929]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C=10.0 with score 0.8963\n"
     ]
    }
   ],
   "source": [
    "logreg_best = [z for z in zip(C, logreg_scores) if z[1]==max(logreg_scores)][0]\n",
    "print(f'Best C={logreg_best[0]} with score {logreg_best[1]}')\n",
    "best_C = logreg_best[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 4: Features (10 points)\n",
    "### Part 4a.\n",
    "We will now stick to the `LogisticRegression()` with the optimal C from the last point and see whether we are able to improve the results further by extending the feature extractor with more features. \n",
    "\n",
    "First, we try adding a feature for the next word in the sentence, and then train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_next_suffix_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"      # start of this sentence set\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]  # previous word in this sentence \n",
    "    \n",
    "    features['token'] = sentence[i]            # add word as feature to data\n",
    "    \n",
    "    if i < len(sentence)-1:                    # make sure i doesn't exceed the indexes of the sentence\n",
    "        features['next-word'] = sentence[i+1] \n",
    "    else:\n",
    "        features['next-word'] = '<END>'\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logreg_tagger = ScikitConsecutivePosTagger(news_train, \n",
    "                                           features=prev_next_suffix_features, \n",
    "                                           clf= LogisticRegression(C=best_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: previous, current, and next words + suffixes, score: 0.9127\n"
     ]
    }
   ],
   "source": [
    "print('Features: previous, current, and next words + suffixes, score: %5.4f' % round(logreg_tagger.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4b.\n",
    "Try to add more features to get an even better tagger. Only the fantasy sets limits to what you may consider. \n",
    "\n",
    "Some candidates include: \n",
    "- Is the word a number? \n",
    "- Is it capitalized? \n",
    "- Does it contain capitals? \n",
    "- Does it contain a hyphen? \n",
    "- Consider larger contexts? \n",
    "\n",
    "\n",
    "What is the best feature set you can come up with? Train and test various feature sets and select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_features(sentence, i, history):\n",
    "    # Suffixes\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    \n",
    "    # Previous word\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"      # start of this sentence set\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]  # previous word in this sentence \n",
    "    \n",
    "    # Current word\n",
    "    features['token'] = sentence[i]            # add word as feature to data\n",
    "    \n",
    "    # Next word\n",
    "    if i < len(sentence)-1:                    # make sure i doesn't exceed the indexes of the sentence\n",
    "        features['next-word'] = sentence[i+1] \n",
    "    else:\n",
    "        features['next-word'] = '<END>'\n",
    "        \n",
    "    # Is number\n",
    "    try:\n",
    "        int(features['token'])\n",
    "    except ValueError:\n",
    "        features['is_numeric'] = False\n",
    "    else:\n",
    "        features['is_numeric'] = True\n",
    "        \n",
    "    # Is capitalized\n",
    "    if features['token'][0].isupper():\n",
    "        features['capitalized'] = True\n",
    "    else:\n",
    "        features['capitalized'] = False\n",
    "        \n",
    "#     # Word length (worsened the model)\n",
    "#     features['word_length'] = len(features['token'])\n",
    "    \n",
    "#     # Sentence length (worsened the model)\n",
    "#     features['sentence_length'] = len(sentence)\n",
    "    \n",
    "#     # Previous tags (didn't work bc of how tag() is written)\n",
    "#     if i==0:\n",
    "#         features['prev-tag'] = '<START>'\n",
    "#     else:\n",
    "#         features['prev-tag'] = history[i-1]\n",
    "#     if i>1:\n",
    "#         features['prev-tag(2)'] = history[i-2]\n",
    "        \n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/3699 [00:00<?, ?it/s]\n",
      " 22%|█████████████████                                                            | 821/3699 [00:00<00:00, 8208.76it/s]\n",
      " 48%|████████████████████████████████████▎                                       | 1768/3699 [00:00<00:00, 8527.32it/s]\n",
      " 70%|█████████████████████████████████████████████████████▎                      | 2595/3699 [00:00<00:00, 8448.38it/s]\n",
      " 95%|████████████████████████████████████████████████████████████████████████▏   | 3514/3699 [00:00<00:00, 8634.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 3699/3699 [00:00<00:00, 8582.54it/s]C:\\Users\\perha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "my_logreg_tagger = ScikitConsecutivePosTagger(news_train,\n",
    "                                              features=my_features,\n",
    "                                              clf= LogisticRegression(C=best_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9262"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(my_logreg_tagger.evaluate(news_dev_test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the best score that I was able to get. It barely best the expanded score above. Surprisingly enough, word and sentence length actually caused the scores to drop dramatically, while checking for capitalized words and numbers increased it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex5: Larger corpus and evaluation (15 points)\n",
    "### Part a.\n",
    "We can now test our best tagger so far on the `news_final_test` set, to see how is the result compares to testing on news_dev_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.923"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(my_logreg_tagger.evaluate(news_final_test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b.\n",
    "But we are looking for bigger fish. How good is our settings when trained on a bigger corpus?\n",
    "\n",
    "We will use nearly the whole Brown corpus. But we will take away two categories for later evaluation: adventure and hobbies. We will also initially stay clear of news to be sure not to mix training and test data.\n",
    "\n",
    "Create a variable `rest` containing the Brown corpus with all categories except these three. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_categories = [c for c in brown.categories() if c not in ['adventure', 'hobbies', 'news']]\n",
    "rest = brown.tagged_sents(categories=selected_categories) # read in corups as sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the tagged sentences from `rest` and remember to use the universal pos tagset. Then split the set into 80%-10%-10%: `rest_train`, `rest_dev_test`, `rest_final_test`. (Why not just 90%-10%?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest = list(rest)\n",
    "np.random.shuffle(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Where', 'WRB'),\n",
       " ('schools', 'NNS'),\n",
       " (',', ','),\n",
       " ('fire', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('police', 'NN'),\n",
       " ('protection', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('similar', 'JJ'),\n",
       " ('municipal', 'JJ'),\n",
       " ('services', 'NNS'),\n",
       " ('are', 'BER'),\n",
       " ('of', 'IN'),\n",
       " ('equal', 'JJ'),\n",
       " ('quality', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('city', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('country', 'NN'),\n",
       " (',', ','),\n",
       " ('real', 'JJ'),\n",
       " ('estate', 'NN'),\n",
       " ('taxes', 'NNS'),\n",
       " ('are', 'BER'),\n",
       " ('usually', 'RB'),\n",
       " ('about', 'RB'),\n",
       " ('the', 'AT'),\n",
       " ('same', 'AP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(rest) * 0.1)\n",
    "\n",
    "rest_train = rest[:-2*size] \n",
    "rest_dev_test = rest[-2*size:-size]\n",
    "rest_final_test = rest[-size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35111 4388 4388\n"
     ]
    }
   ],
   "source": [
    "print(len(rest_train), len(rest_dev_test), len(rest_final_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then merge these three sets with the corresponding sets from news to get final training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rest_train+news_train\n",
    "dev   = rest_dev_test + news_dev_test\n",
    "test  = rest_final_test + news_final_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to establish a new baseline. This can be done the same way as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_tagger = UnigramTagger(train)\n",
    "\n",
    "baseline = [\n",
    "    [(word, tag) if tag!=None else (word, most_common)\n",
    "     for word, tag in baseline_tagger.tag(nltk.tag.untag(sentence))]\n",
    "     for sentence in dev \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(len(dev)):\n",
    "    for w in range(len(dev[i])):\n",
    "        if dev[i][w] == baseline[i][w]:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New baseline:  0.8999\n"
     ]
    }
   ],
   "source": [
    "print('New baseline: ', round(np.mean(results), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c.\n",
    "We can then build our tagger for this larger domain. Use the best settings from the earlier exercises, train on train and test on test. What is the accuracy of your tagger?\n",
    "\n",
    "Warning: Running this experiment may take 15-30 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/38810 [00:00<?, ?it/s]\n",
      "  0%|                                                                              | 1/38810 [00:00<7:11:39,  1.50it/s]\n",
      "  2%|█▎                                                                          | 689/38810 [00:00<4:56:50,  2.14it/s]\n",
      "  4%|███▏                                                                       | 1625/38810 [00:00<3:22:42,  3.06it/s]\n",
      "  7%|█████▍                                                                     | 2788/38810 [00:00<2:17:28,  4.37it/s]\n",
      " 10%|███████▊                                                                   | 4012/38810 [00:01<1:32:58,  6.24it/s]\n",
      " 14%|██████████▌                                                                | 5465/38810 [00:01<1:02:22,  8.91it/s]\n",
      " 18%|█████████████▊                                                               | 6964/38810 [00:01<41:42, 12.72it/s]\n",
      " 22%|████████████████▉                                                            | 8560/38810 [00:01<27:44, 18.17it/s]\n",
      " 26%|███████████████████▊                                                        | 10145/38810 [00:01<18:24, 25.95it/s]\n",
      " 30%|██████████████████████▉                                                     | 11719/38810 [00:01<12:11, 37.04it/s]\n",
      " 34%|█████████████████████████▊                                                  | 13175/38810 [00:01<08:04, 52.86it/s]\n",
      " 38%|████████████████████████████▊                                               | 14685/38810 [00:01<05:19, 75.40it/s]\n",
      " 42%|███████████████████████████████▎                                           | 16175/38810 [00:01<03:30, 107.48it/s]\n",
      " 45%|██████████████████████████████████                                         | 17639/38810 [00:02<02:18, 152.82it/s]\n",
      " 49%|████████████████████████████████████▋                                      | 18957/38810 [00:02<01:31, 216.98it/s]\n",
      " 53%|███████████████████████████████████████▋                                   | 20524/38810 [00:02<00:59, 308.14it/s]\n",
      " 57%|██████████████████████████████████████████▉                                | 22212/38810 [00:02<00:38, 436.74it/s]\n",
      " 61%|█████████████████████████████████████████████▋                             | 23651/38810 [00:02<00:24, 615.20it/s]\n",
      " 65%|████████████████████████████████████████████████▌                          | 25134/38810 [00:02<00:15, 863.35it/s]\n",
      " 69%|██████████████████████████████████████████████████▋                       | 26595/38810 [00:02<00:10, 1202.60it/s]\n",
      " 72%|█████████████████████████████████████████████████████▍                    | 28025/38810 [00:02<00:06, 1657.64it/s]\n",
      " 76%|████████████████████████████████████████████████████████▍                 | 29568/38810 [00:02<00:04, 2263.82it/s]\n",
      " 80%|███████████████████████████████████████████████████████████▏              | 31029/38810 [00:03<00:02, 2939.36it/s]\n",
      " 83%|█████████████████████████████████████████████████████████████▋            | 32342/38810 [00:03<00:01, 3726.97it/s]\n",
      " 87%|████████████████████████████████████████████████████████████████▌         | 33830/38810 [00:03<00:01, 4803.46it/s]\n",
      " 91%|███████████████████████████████████████████████████████████████████       | 35175/38810 [00:03<00:00, 5943.56it/s]\n",
      " 94%|█████████████████████████████████████████████████████████████████████▌    | 36481/38810 [00:03<00:00, 6040.35it/s]\n",
      " 97%|███████████████████████████████████████████████████████████████████████▋  | 37584/38810 [00:03<00:00, 6370.80it/s]\n",
      " 99%|█████████████████████████████████████████████████████████████████████████▌| 38575/38810 [00:03<00:00, 6797.57it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 38810/38810 [00:03<00:00, 10027.32it/s]"
     ]
    }
   ],
   "source": [
    "# Because of memory error, I had to use the SGD library to implement the logistic regression.\n",
    "# This allowed my to train my model in batches, thus avoiding the memory error.\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "big_tagger = ScikitConsecutivePosTagger(train,\n",
    "                                        features=my_features,\n",
    "                                        clf= SGDClassifier(loss='log',\n",
    "                                                           alpha=1./best_C,\n",
    "                                                           warm_start=True))\n",
    "                                                          \n",
    "\n",
    "# big_tagger = ScikitConsecutivePosTagger(train,\n",
    "#                                         features=my_features,\n",
    "#                                         clf= LogisticRegression(C=best_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOPEFULLY THERE ARE RESULTS HERE!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5311"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(big_tagger.evaluate(dev), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score should have been much higher, compare to some fellow students' results. This probably has to do with the change in classifier I used to try and solve my memory problem. I guess it didn't pay off..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex6: Comparing to other taggers (10 points)\n",
    "### Part a.\n",
    "In the lectures, we spent quite some time on the HMM-tagger. NLTK comes with an HMM-tagger which we may train and test on our own corpus. It can be trained by\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_hmm_tagger = nltk.HiddenMarkovModelTagger.train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and tested similarly as we have tested our other taggers. Train and test it, first on the news set then on the big train/test set. How does it perform compared to your best tagger? What about speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-d27adb2b5638>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hmm score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnews_hmm_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'seconds'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\api.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, gold)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \"\"\"\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mtagged_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muntag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mgold_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mtest_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\api.py\u001b[0m in \u001b[0;36mtag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\hmm.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, unlabeled_sequence)\u001b[0m\n\u001b[0;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m    293\u001b[0m         \u001b[0munlabeled_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munlabeled_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munlabeled_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munlabeled_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\hmm.py\u001b[0m in \u001b[0;36m_tag\u001b[1;34m(self, unlabeled_sequence)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munlabeled_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_best_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munlabeled_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munlabeled_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\hmm.py\u001b[0m in \u001b[0;36m_best_path\u001b[1;34m(self, unlabeled_sequence)\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munlabeled_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m         \u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\hmm.py\u001b[0m in \u001b[0;36m_update_cache\u001b[1;34m(self, symbols)\u001b[0m\n\u001b[0;32m    363\u001b[0m                 \u001b[1;31m# add new columns to the output probability table without\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m                 \u001b[1;31m# destroying the old probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m                 \u001b[0mO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                     \u001b[0msi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "print('hmm score', news_hmm_tagger.evaluate(dev))\n",
    "print(round(time.time()-st, 4), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time i ran this it took 224 seconds, but I unfortunately did not store the score. Since I keep getting a memory error, I cannot compare this tagger to my best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part b\n",
    "NLTK also comes with an averaged perceptron tagger which we may train and test. It is currently considered the best tagger included with NLTK. It can be trained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "per_tagger = nltk.PerceptronTagger(load=False)\n",
    "per_tagger.train(train)\n",
    "print('Training perceptron took', time.time()-st,  'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "print('Preceptron score:', per_tagger.evaluate(dev))\n",
    "print(round(time.time()-st, 4), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is tested similarly to our other taggers.\n",
    "\n",
    "Train and test it, first on the news set and then on the big train/test set. How does it perform compared to your best tagger? Did you beat it? What about speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also ran into memory problems here. For this last execution, I never managed to get it to finish running. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
